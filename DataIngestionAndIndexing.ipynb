{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "# Read local .env file for environment variables\n",
    "# We use OpenAI during embedding so you need to save your OpenAI API Key in the .env file in the same directory as this script\n",
    "_ = load_dotenv(find_dotenv())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_file_path):\n",
    "    \"\"\"\n",
    "    Load the JSON data from the specified file.\n",
    "\n",
    "    Args:\n",
    "        json_file_path (str): Path to the JSON file containing blog posts.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(content):\n",
    "    \"\"\"\n",
    "    Remove HTML tags from the content using BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        content (str): The original HTML content.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned content with HTML tags removed.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    return soup.get_text(separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_func(record, metadata):\n",
    "    \"\"\"\n",
    "    Extract metadata from a record.\n",
    "\n",
    "    Args:\n",
    "        record (dict): The original record data.\n",
    "        metadata (dict): An empty or existing metadata dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated metadata dictionary with extracted values.\n",
    "    \"\"\"\n",
    "    metadata[\"title\"] = record.get(\"title\")\n",
    "    metadata[\"published_date\"] = record.get(\"published_date\")\n",
    "    metadata[\"url\"] = record.get(\"url\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the data\n",
    "json_file_path = \"all_posts.json\"  # Placeholder for the JSON file path: path/to/your/all_posts.json\n",
    "published_posts = load_data(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no empty lists in categories or tags\n",
    "for post in published_posts[\"posts\"]:\n",
    "    post[\"categories\"] = post.get(\"categories\", [\"None\"])\n",
    "    post[\"tags\"] = post.get(\"tags\", [\"None\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each post content of irrelevant HTML tags\n",
    "for post in published_posts[\"posts\"]:\n",
    "    post[\"cleaned_content\"] = clean_html(post[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample post after cleaning:\n",
      "Title: Zion National Park guide for First-timer Families with kids\n",
      "Content: \n",
      " Nestled in the heart of Utah's canyon country, Zion National Park offers a stunning array of red r...\n"
     ]
    }
   ],
   "source": [
    "# Print a sample post for verification\n",
    "sample_post = published_posts[\"posts\"][0]\n",
    "print(f\"Sample post after cleaning:\\nTitle: {sample_post['title']}\\nContent: {sample_post['cleaned_content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned content saved back to JSON file.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned content back to the JSON file\n",
    "with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(published_posts, f, ensure_ascii=False, indent=4)\n",
    "print(\"Cleaned content saved back to JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using the JSONLoader with specified configurations\n",
    "loader = JSONLoader(\n",
    "    file_path=json_file_path,\n",
    "    jq_schema='.posts[]',  # Adjust according to your JSON structure\n",
    "    content_key=\"cleaned_content\",\n",
    "    metadata_func=metadata_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents.\n"
     ]
    }
   ],
   "source": [
    "# Load the documents from the loader and print a sample\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the token splitter with specific configurations\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=0,  # Overlap between chunks\n",
    "    tokens_per_chunk=256  # Number of tokens per chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document splits: 30\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into chunks based on tokens\n",
    "all_splits = token_splitter.split_documents(documents)\n",
    "print(f\"Total document splits: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/Users/chandler/Documents/chatbot_public/all_posts.json',\n",
       " 'seq_num': 3,\n",
       " 'title': 'What I am (still) grateful for after 2 years in the US',\n",
       " 'published_date': 'Thu, 14 Dec 2023 03:30:00 +0000',\n",
       " 'url': 'https://www.chandlernguyen.com/blog/2023/12/13/what-i-am-still-grateful-for-after-2-years-in-the-us/'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample meta data from a chunk to check\n",
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and FAISS vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector store locally\n",
    "db.save_local(\"path/to/save/faiss_index\")  # Placeholder for save path and index name. Change to your preference. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
